Data Generation

- need dependency parsed and pos-tagged data
- idea: run a dependency parser/pos tagger over wikipedia data, translate entire sentence, then
mix and match PP/NP/CP/VPs

8 multilingual combos of subj, attractor, predicate.

Start with a sentence with POS tags.

Experiment 1 (Production): Predict Number:
Data: sentence up to but not including the verb
Label: pos of verb (VBZ or VBP); binary classification

Experiment 2 (Comprehension): Is Grammatical:
Data: sentences, half the time (randomly) change inflection of verb (sing to plural or vice versa)
Label: grammatical or not (i.e. whether inflection changed); binary classification

Models will have to have been trained on both languages (unsure if we should do it with monolingual examples, and then test codeswitching only in test, or if we should train on cs examples. Perhaps both?)

Test on general language model and also on model trained specifically for the 2 tasks.

Questions:
1. Why do we need dependency parsing?


Pipeline:
1. Get Wikipedia text - FAIR's colorless green rnn's repo
2. Get dependency parsing - Spacy/Stanford core NLP
3. Get POS tagging - Spacy
OR
1. Get Linzen's RNNs as psycholinguistic subjects data

4. Get translations - Inflect verbs using PyPi pluraliser

References:
1. https://github.com/SArehalli/NLMsCaptureSomeAgrAttr
2. https://github.com/TalLinzen/rnn_agreement
3. https://github.com/yoavg/bert-syntax
4. https://github.com/facebookresearch/colorlessgreenRNNs/tree/4ffcabc991c866608aeed2ba35059a458ff2845f/data
